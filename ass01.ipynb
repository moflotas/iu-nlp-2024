{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKboZnAdgrRM"
      },
      "source": [
        "# [NLP] Assignment 1: Tokenization\n",
        "\n",
        "In this assignment, you need to tokenize the text of the Twitter(X) users posts(tweets). The assignment consists of two tasks. When you finish all the tasks, create a GitHub repository for this assignment (you can use this repo later for the other assignments) and submit this notebook in the repository. Leave `requirements.txt` file if your code requires additional installations. Submit the link to the repository in Moodle.\n",
        "\n",
        "The [data](https://drive.google.com/file/d/15x_wPAflvYQ2Xh38iNQGrqUIWLj5l5Nw/view?usp=share_link) contains 5 files whereby each contains 44 tweets. Each tweet is separated by a newline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLDjjAvemUP_"
      },
      "source": [
        "## Task 1. Tokenize some tweets manually (20 points)\n",
        "\n",
        "As a first task you need to tokenize first 15 tweets from `file2` by hand. This will allow you to understand the problem from a linguistic point of view. The guidelines for tweet tokenization are as follows:\n",
        "\n",
        "- Each smiley is a separate token\n",
        "- Each hashtag is an individual token. Each user reference is an individual token\n",
        "- If a word has spaces between them then it is converted to a single token\n",
        "- All punctuations are individual tokens. This includes double-quotes and single quotes also\n",
        "- A URL is a single token\n",
        "\n",
        "Example of output\n",
        "\n",
        "    Input tweet\n",
        "    @xfranman Old age has made N A T O!\n",
        "\n",
        "    Tokenized tweet (separated by comma)\n",
        "    @xfranman , Old , age , has , made , NATO , !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KKKwTidnzUw"
      },
      "source": [
        "\n",
        "    1. Input tweet\n",
        "    ...\n",
        "    1. Tokenized tweet\n",
        "    ...\n",
        "\n",
        "    2. Input tweet\n",
        "    ...\n",
        "    2. Tokenized tweet\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2J2AD2nmUhi"
      },
      "source": [
        "## Task 2. Implement [Byte-Pair Encoding(BPE)](https://arxiv.org/pdf/1508.07909.pdf) Tokenizer (80 points)\n",
        "\n",
        "### Task 2.1. Implementation (60 points)\n",
        "\n",
        "Implement the tokenizer as the BPETokenizer class:\n",
        "* Implement `train` method that learns merges and builds the vocabulary of the specified `vocab_size` (25 points).\n",
        "* Implement `tokenize` method that should tokenize the text according to the learnt merges (25 points).\n",
        "\n",
        "Your code should have docstrings and comments (10 points)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "class BPETokenizer:\n",
        "\n",
        "    def __init__(self, vocab_size: int) -> None:\n",
        "        self.vocab_size = vocab_size\n",
        "        ...\n",
        "\n",
        "\n",
        "    def train(self, corpus: List[str]) -> None:\n",
        "        ...\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.2. Analysis on Tweets Dataset (10 points)\n",
        "\n",
        "Train the BPE tokenizer on the tweets dataset. Try to tokenize the tweets with the tokenizer of different `vocab_size`. For example, train the BPE tokenizer with `vocab_size` of [base_vocab_size, 250, 500, 750, 1000]. Plot the dependency of the average length of the tokenized tweet by `vocab_size` to analyze how `vocab_size` affects the length of the tokenized tweet on average. Tell what `vocab_size` is preferrable and why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.3. Analysis on Dataset of Different Language (10 points)\n",
        "\n",
        "Find a small dataset of texts in a language other than English. The dataset size should be not greater than several megabytes.\n",
        "\n",
        "Train the BPE tokenizer on the dataset that you found. Try to tokenize the sentences from this dataset with the tokenizer of different `vocab_size`. Plot the dependency of the average length of the tokenized sentence by `vocab_size` to analyze how `vocab_size` affects the length of the tokenized sentence on average.\n",
        "\n",
        "Tell how how the average length of the tokenized sentence differs from the average length of the tokenized tweet. Explain why. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
