{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvig’s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def words(text):\n",
        "    return re.findall(r\"\\w+\", text.lower())\n",
        "\n",
        "\n",
        "WORDS = Counter(words(open(\"data/big.txt\", encoding=\"unicode_escape\").read()))\n",
        "\n",
        "\n",
        "def P(word, N=sum(WORDS.values())):\n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "\n",
        "def correction(word):\n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "\n",
        "def candidates_with_prob(word):\n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return [\n",
        "        (c, P(c))\n",
        "        for c in (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "    ]\n",
        "\n",
        "\n",
        "def candidates(word):\n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return known([word]) or known(edits1(word)) or known(edits2(word)) or [word]\n",
        "\n",
        "\n",
        "def known(words):\n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    deletes = [L + R[1:] for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
        "    inserts = [L + c + R for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "\n",
        "def edits2(word):\n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('doing', 0.00015872562937387197),\n",
              " ('king', 0.00021133693349217785),\n",
              " ('dying', 7.133736151634695e-05)]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# quick test\n",
        "candidates_with_prob(\"dking\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## My implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The main problem with Norvig's solution, is that it does not take context words in an account. So this is simple greedy approach, which works, but not that good. We should expect accuracy of bigram model be at least as good, as Norvig's solution, and five-gram model, to outperform them both."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Store all context\n",
        "My first idea was to generate all variations of context, generate all variations of next word (which we predicted to be correct) and store them in model. So, e.g. if my bigram dataset is \n",
        "\n",
        "```csv\n",
        "4 what is\n",
        "6 the strongest\n",
        "```\n",
        "\n",
        "then, I would store a map like this:\n",
        "\n",
        "```python\n",
        "map[\"the\"][\"strongest\"] = 6\n",
        "map[\"thea\"][\"strongest\"] = 6\n",
        "map[\"theb\"][\"strongest\"] = 6\n",
        "map[\"thec\"][\"strongest\"] = 6\n",
        "# ... all possible context and word edits\n",
        "```\n",
        "\n",
        "The problem with this approach, is there are simply too much contexts to generate and store, which results in a very long computation times. I dropped this idea immediately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate context during inference\n",
        "\n",
        "Second idea was to do basically the same, but iterate over all contexts and next words during inference. And I wrote code for this approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1020363</th>\n",
              "      <td>24</td>\n",
              "      <td>zviad</td>\n",
              "      <td>gamsakhurdia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1020364</th>\n",
              "      <td>25</td>\n",
              "      <td>zweimal</td>\n",
              "      <td>leben</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1020365</th>\n",
              "      <td>24</td>\n",
              "      <td>zwick</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1020366</th>\n",
              "      <td>24</td>\n",
              "      <td>zydeco</td>\n",
              "      <td>music</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1020367</th>\n",
              "      <td>72</td>\n",
              "      <td>zz</td>\n",
              "      <td>top</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0        1             2\n",
              "1020363  24    zviad  gamsakhurdia\n",
              "1020364  25  zweimal         leben\n",
              "1020365  24    zwick           and\n",
              "1020366  24   zydeco         music\n",
              "1020367  72       zz           top"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# keep_default_na in order for \"null\" to be converted to None\n",
        "bigram = pd.read_csv(\"data/w2.txt\", sep=\"\\t\", header=None, keep_default_na=False)\n",
        "fivegram = pd.read_csv(\"data/w5.txt\", sep=\"\\t\", header=None, keep_default_na=False)\n",
        "bigram.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigram.isna().sum().sum(), fivegram.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "from functools import cache\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "@cache\n",
        "def get_edits(word: str):\n",
        "    if len(word) <= 2:\n",
        "        return {word}\n",
        "\n",
        "    return {word} | edits1(word)\n",
        "\n",
        "\n",
        "class NGramModel:\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        # Create a placeholder for model\n",
        "        model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "        # Count frequency of co-occurance\n",
        "        t = tqdm(total=df.shape[0])\n",
        "        for _, ngram in df.iterrows():\n",
        "            t.update()\n",
        "            count, *words = ngram\n",
        "            *context, word = words\n",
        "\n",
        "            model[tuple(sorted(context))][word] += count\n",
        "\n",
        "        # Let's transform the counts to probabilities\n",
        "        for context in model:\n",
        "            total_count = float(sum(model[context].values()))\n",
        "            for suggestion in model[context]:\n",
        "                model[context][suggestion] /= total_count\n",
        "\n",
        "        self.model = model\n",
        "        self.context_len = len(context)\n",
        "\n",
        "    def suggest(self, context: list[str], word: str) -> str:\n",
        "        best_suggestion = None\n",
        "        best_prob = 0\n",
        "\n",
        "        # if context is too short, we can't make any suggestions\n",
        "        context = [item if item else \"<UNK>\" for item in context]\n",
        "        context = context[-self.context_len :]\n",
        "\n",
        "        # get all possible contexts\n",
        "        all_contexts = lambda: product(*[get_edits(c) for c in sorted(context)])\n",
        "\n",
        "        # if the word exists, then it is very probable to be correct\n",
        "        for probable_context in all_contexts():\n",
        "            prob = self.model[tuple(probable_context)][word]\n",
        "            if 0.95 * prob > best_prob:\n",
        "                best_prob = 0.95 * prob\n",
        "\n",
        "        # but, it might happen, that changes of this word might be more probable\n",
        "        for probable_context in all_contexts():\n",
        "            for suggestion in get_edits(word):\n",
        "                prob = self.model[tuple(probable_context)][suggestion]\n",
        "                if 0.05 * prob > best_prob:\n",
        "                    best_prob = 0.05 * prob\n",
        "                    best_suggestion = suggestion\n",
        "\n",
        "        if best_suggestion is None:\n",
        "            return sorted(candidates_with_prob(word), key=lambda x: x[1])[-1]\n",
        "\n",
        "        return best_suggestion, best_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1020368 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1020368/1020368 [00:34<00:00, 29270.49it/s]\n"
          ]
        }
      ],
      "source": [
        "model = NGramModel(bigram)\n",
        "# model.model = d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('rich', 8.2037965743799e-05)\n",
            "('man', 0.0002760255055068708)\n",
            "('values', 0.0011465603190428714)\n",
            "('to', 0.0016368481157213552)\n",
            "('psychologist', 1.6330975460692016e-07)\n",
            "('king', 0.00021133693349217785)\n",
            "('king', 0.00021133693349217785)\n"
          ]
        }
      ],
      "source": [
        "# random context uses Norvig's suggestion\n",
        "print(model.suggest([None], \"fich\"))\n",
        "\n",
        "print(model.suggest([\"good\"], \"maan\"))\n",
        "print(model.suggest([\"defalt\"], \"vales\"))\n",
        "\n",
        "print(model.suggest([\"psychologyst\"], \"too\"))\n",
        "print(model.suggest([\"too\"], \"psychologyst\"))\n",
        "\n",
        "print(model.suggest([\"sport\"], \"dking\"))\n",
        "print(model.suggest([\"species\"], \"dking\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unfortunately, because there are no bigrams `doing sport` or `dying species` in the `w2.txt` dataset, the model cannot correct them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I tried testing with both `w2.txt` and `w5.txt` dataset, and, due to the exponential growth of `probable_context` with the number of ngrams, inference for `w5.txt` takes simply too much time. That is why I didn't use it. This is because this algorithm is not very efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can clearly see, that I am using edit distance with weights of `0.95` for word without correction and `0.05` for word with 1 correction. This is because if the word exists in the dataset, then it is very likely, that this word is correct. However, it might happend, that this word does not exist in dataset or the correction will have hight probability to occur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I decided to use `github-corpus` dataset, because I found it very easy to work with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>repo</th>\n",
              "      <th>commit</th>\n",
              "      <th>message</th>\n",
              "      <th>edits</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://github.com/abacritt/angularx-social-login</td>\n",
              "      <td>d4c912f5ccd70c81f424fadbf1fe1a2ecb942f07</td>\n",
              "      <td>Fix typo\\n</td>\n",
              "      <td>[{'src': {'text': '            IN.User.authori...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://github.com/abacritt/angularx-social-login</td>\n",
              "      <td>8beb5a5ebee0882142541dc84c004f6ce3165f94</td>\n",
              "      <td>fix typo\\n\\nfix typo in firstname\\n</td>\n",
              "      <td>[{'src': {'text': '                    user.em...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://github.com/abahmed/Deer</td>\n",
              "      <td>44781b8842c7e647d1f5d2417d21244e815c5eec</td>\n",
              "      <td>fixed typo (#263)\\n\\n</td>\n",
              "      <td>[{'src': {'text': ' :de:             | Deutsch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://github.com/abakan/ablog</td>\n",
              "      <td>1cee106975e3137cb9a667729e832cc9494f0692</td>\n",
              "      <td>Fixed typo.\\n</td>\n",
              "      <td>[{'src': {'text': '        :nocoments:', 'path...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://github.com/abakan/ablog</td>\n",
              "      <td>4e11caf1f7ebe611ffb08f8a6909ac6752d784cd</td>\n",
              "      <td>Fixed typo\\n</td>\n",
              "      <td>[{'src': {'text': '   You can disable comments...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                repo  \\\n",
              "0  https://github.com/abacritt/angularx-social-login   \n",
              "1  https://github.com/abacritt/angularx-social-login   \n",
              "2                    https://github.com/abahmed/Deer   \n",
              "3                    https://github.com/abakan/ablog   \n",
              "4                    https://github.com/abakan/ablog   \n",
              "\n",
              "                                     commit  \\\n",
              "0  d4c912f5ccd70c81f424fadbf1fe1a2ecb942f07   \n",
              "1  8beb5a5ebee0882142541dc84c004f6ce3165f94   \n",
              "2  44781b8842c7e647d1f5d2417d21244e815c5eec   \n",
              "3  1cee106975e3137cb9a667729e832cc9494f0692   \n",
              "4  4e11caf1f7ebe611ffb08f8a6909ac6752d784cd   \n",
              "\n",
              "                               message  \\\n",
              "0                           Fix typo\\n   \n",
              "1  fix typo\\n\\nfix typo in firstname\\n   \n",
              "2                fixed typo (#263)\\n\\n   \n",
              "3                        Fixed typo.\\n   \n",
              "4                         Fixed typo\\n   \n",
              "\n",
              "                                               edits  \n",
              "0  [{'src': {'text': '            IN.User.authori...  \n",
              "1  [{'src': {'text': '                    user.em...  \n",
              "2  [{'src': {'text': ' :de:             | Deutsch...  \n",
              "3  [{'src': {'text': '        :nocoments:', 'path...  \n",
              "4  [{'src': {'text': '   You can disable comments...  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gh_df = pd.read_json(\"data/github-corpus.json\", lines=True)\n",
        "gh_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_test_data(amount: int = 200):\n",
        "    test_data = []\n",
        "    for _, item in gh_df.iterrows():\n",
        "        for test_case in item[\"edits\"]:\n",
        "            if not \"is_typo\" in test_case:\n",
        "                continue\n",
        "            if test_case[\"is_typo\"] and test_case[\"src\"][\"path\"].endswith(\".md\"):\n",
        "                src = words(test_case[\"src\"][\"text\"])\n",
        "                tgt = words(test_case[\"tgt\"][\"text\"])\n",
        "\n",
        "                if len(src) != len(tgt) or src == tgt:\n",
        "                    continue\n",
        "\n",
        "                indecies = [i for i in range(len(src)) if src[i] != tgt[i]]\n",
        "                for index in indecies:\n",
        "                    test_data.append((src, tgt, index))\n",
        "                    if len(test_data) >= amount:\n",
        "                        break\n",
        "\n",
        "    return test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['de', 'deutsche']\n",
            "['de', 'deutsch']\n",
            "1\n",
            "\n",
            "['add', 'watchdog', '0', 'to', 'disablle', 'watchdog', 'timer', 'if', 'you', 'get', 'accidental', 'reboots']\n",
            "['add', 'watchdog', '0', 'to', 'disable', 'watchdog', 'timer', 'if', 'you', 'get', 'accidental', 'reboots']\n",
            "4\n",
            "\n",
            "['generates', 'a', 'random', 'hecadecimal', 'color', 'code']\n",
            "['generates', 'a', 'random', 'hexadecimal', 'color', 'code']\n",
            "3\n",
            "\n",
            "['takes', 'a', 'veriadic', 'function', 'and', 'returns', 'a', 'closure', 'that', 'accepts', 'an', 'array', 'of', 'arguments', 'to', 'map', 'to', 'the', 'inputs', 'of', 'the', 'function']\n",
            "['takes', 'a', 'variadic', 'function', 'and', 'returns', 'a', 'closure', 'that', 'accepts', 'an', 'array', 'of', 'arguments', 'to', 'map', 'to', 'the', 'inputs', 'of', 'the', 'function']\n",
            "2\n",
            "\n",
            "['checks', 'if', 'the', 'provided', 'intiger', 'is', 'primer', 'number']\n",
            "['checks', 'if', 'the', 'provided', 'intiger', 'is', 'prime', 'number']\n",
            "6\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_data = generate_test_data()\n",
        "print(*[\"{}\\n{}\\n{}\\n\".format(*test_case) for test_case in test_data][:5], sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_my_solution(model, test_data):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for src, tgt, index in tqdm(test_data):\n",
        "        word = src[index]\n",
        "        corr = tgt[index]\n",
        "\n",
        "        pred, _ = model.suggest(src[:index], word)\n",
        "\n",
        "        if pred == corr:\n",
        "            correct += 1\n",
        "\n",
        "        total += 1\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "def evaluate_norvigs_solution(test_data):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for src, tgt, index in tqdm(test_data):\n",
        "        word = src[index]\n",
        "        corr = tgt[index]\n",
        "\n",
        "        pred = sorted(candidates_with_prob(word), key=lambda x: x[1])[-1][0]\n",
        "\n",
        "        if pred == corr:\n",
        "            correct += 1\n",
        "\n",
        "        total += 1\n",
        "\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/201 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 201/201 [00:11<00:00, 16.88it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.39800995024875624"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_my_solution(model, test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 201/201 [00:06<00:00, 29.09it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.3383084577114428"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_norvigs_solution(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, bigram solution using my implementation outperforms the Norvig's solution on github dataset. It is a bit slower and with increasing context it will require exponentially more time to compute, but this is the cost of accuracy in my implementation."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
